{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c478a906",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "Unsupervised learning is a type of Machine Learning where the model learns patterns from data without labels.\n",
    "- Input → X only (no target y)\n",
    "- Model finds hidden structure:\n",
    "    - Groups\n",
    "    - Patterns\n",
    "    - Relationships\n",
    "\n",
    "✔ One-line definition:\n",
    "Unsupervised learning discovers hidden patterns in unlabeled data.\n",
    "\n",
    "Clustering: Grouping similar data points.\n",
    "\n",
    "Common Algorithms:\n",
    "-   K-Means\n",
    "-   Hierarchical Clustering\n",
    "-   DBSCAN\n",
    "\n",
    "Real-life Uses:\n",
    "- Customer Segmentation\n",
    "- Market Segmentation\n",
    "- Document Clustering\n",
    "- Image Segmentation\n",
    "\n",
    "Dimensionality Reduction\n",
    "Reducing the number of features while keeping important information.\n",
    "\n",
    "Algorithms:\n",
    "- PCA (Principal Component Analysis)\n",
    "- t-SNE\n",
    "- Autoencoders\n",
    "\n",
    "Uses:\n",
    "- Compress images\n",
    "- Speed up ML models\n",
    "- Visualization of high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61f51e",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "How K-Means works (simple steps)\n",
    "- Choose number of clusters K\n",
    "- Select K centroids randomly\n",
    "- Assign each point to the nearest centroid\n",
    "- Recalculate centroids\n",
    "- Repeat until stable\n",
    "\n",
    "Important parameters:\n",
    "- n_clusters → number of groups\n",
    "- random_state → fix randomness\n",
    "- n_init → run multiple times and choose best\n",
    "\n",
    "| Term         | Meaning                           |\n",
    "| ------------ | --------------------------------- |\n",
    "| **Cluster**  | Group of similar data points      |\n",
    "| **Centroid** | Center of a cluster (mean point)  |\n",
    "| **Inertia**  | How far points are from centroids |\n",
    "\n",
    "Elbow Method (Choosing Best K):\n",
    "Elbow Method helps find the optimal number of clusters.\n",
    "Steps:\n",
    "- Run K-Means for K = 1 to 10\n",
    "- Calculate inertia for each\n",
    "- Plot K vs inertia\n",
    "- The point where curve bends like an elbow is the best K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c693e",
   "metadata": {},
   "source": [
    "#### Summary Table\n",
    "| Concept   | Supervised                       | Unsupervised                          |\n",
    "| --------- | -------------------------------- | ------------------------------------- |\n",
    "| Input     | X + y                            | Only X                                |\n",
    "| Goal      | Predict output                   | Find patterns                         |\n",
    "| Examples  | Regression, Classification       | K-Means, PCA                          |\n",
    "| Use Cases | Score prediction, Spam detection | Customer segmentation, Topic modeling |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6b587",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "PCA is a dimensionality reduction technique that converts high-dimensional data into fewer dimensions while keeping maximum important information.\n",
    "\n",
    "One-line definition:\n",
    "PCA reduces the number of features by creating new features called principal components.\n",
    "\n",
    "Why Do We Use PCA?\n",
    "- When datasets have too many features, ML models may become:\n",
    "- Slow\n",
    "- Hard to train\n",
    "- Overfitted\n",
    "- Hard to visualize\n",
    "\n",
    "PCA helps by:\n",
    "- Removing noise\n",
    "- Reducing correlated features\n",
    "- Speeding up training\n",
    "- Improving visualization\n",
    "\n",
    "What PCA Actually Does (Simple Explanation)\n",
    "PCA:\n",
    "1. Finds directions (axes) where the data varies the most\n",
    "2. These directions are called principal components (PCs)\n",
    "3. Projects the data onto these new axes\n",
    "4. Keeps only the most important components\n",
    "5. Removes less important ones\n",
    "\n",
    "Key Terms You Must Know\n",
    "| Term                         | Meaning                                           |\n",
    "| ---------------------------- | ------------------------------------------------- |\n",
    "| **Principal Component (PC)** | New feature created by PCA                        |\n",
    "| **PC1**                      | Direction with maximum variance (most important)  |\n",
    "| **PC2**                      | Second most important direction                   |\n",
    "| **Variance**                 | Spread of data (more variance = more information) |\n",
    "| **Explained Variance Ratio** | % of information kept by each component           |\n",
    "\n",
    "\n",
    "PCA Workflow (Simple Steps)\n",
    "- Step 1: Standardize data\n",
    "(Important because PCA is affected by scale)\n",
    "- Step 2: Calculate covariance matrix\n",
    "Shows relationships between features\n",
    "- Step 3: Find eigenvalues and eigenvectors\n",
    "Eigenvectors → principal components\n",
    "Eigenvalues → importance\n",
    "- Step 4: Sort components by variance\n",
    "PC1 > PC2 > PC3…\n",
    "- Step 5: Keep only top K components\n",
    "New reduced dataset formed\n",
    "\n",
    "When to Use PCA?\n",
    "Use when:\n",
    "- Too many features\n",
    "- High correlation among features\n",
    "- Need visualization (2D/3D)\n",
    "- Speed up ML model\n",
    "\n",
    "Don’t use when:\n",
    "- Features must be interpretable\n",
    "- Non-linear patterns exist (use t-SNE, UMAP)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
