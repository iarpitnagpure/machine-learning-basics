{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c60167",
   "metadata": {},
   "source": [
    "### Evaluation/Prediction Metrics\n",
    "Prediction metrics are how we measure how well a model performs.\n",
    "They depend on the type of problem — Regression (predicting numbers) or Classification (predicting labels).\n",
    "\n",
    "1. Regression Metrics (for predicting continuous values)\n",
    "- MAE (Mean Absolute Error):\n",
    "    - Meaning: Average of the absolute difference between actual and predicted values (ignores direction of error).\n",
    "    - Formula: y_{true} - y_{pred} [1] + y_{true} - y_{pred} [2] + ... \\ total record (Assume negative value to positive)\n",
    "    - Ideal Value: y_{true} - y_{pred}\n",
    "\n",
    "- MSE (Mean Squared Error):\n",
    "    - Meaning: Measures the average of the squared differences between actual and predicted values.\n",
    "    - Formula: s(y_{true} - y_{pred})^2 + (y_{true} - y_{pred})^2 + ... \\ total record (Assume negative value to positive)\n",
    "    - Ideal Value: 0\n",
    "\n",
    "- RMSE (Root Mean Absolute Error):\n",
    "    - Meaning: Square root of MSE (same units as target)\n",
    "    - Formula: sqrt(MSE)\n",
    "    - Ideal Value: 0\n",
    "\n",
    "- R² Score (Coefficient of Determination):\n",
    "    - Meaning: How much variance in target is explained by model\n",
    "    - Formula: 1−SStot​/SSres​​\n",
    "    - Ideal Value: 1 (best), can be negative\n",
    "\n",
    "\n",
    "\n",
    "2. Classification Metrics (for predicting categories or classes)\n",
    "- Accuracy:\n",
    "    - Meaning: % of correct predictions\n",
    "    - Use Case: Balanced datasets\n",
    "    - Ideal Value: 1.0 (100%)\n",
    "\n",
    "- Precision:\n",
    "    - Meaning: Of all predicted positives, how many are actually positive\n",
    "    - Use Case: When False Positives matter (e.g. spam filter)\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- Recall (Sensitivity):\n",
    "    - Meaning: Of all actual positives, how many did we catch\n",
    "    - Use Case: When False Negatives matter (e.g. disease detection)\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- F1-Score:\n",
    "    - Meaning: Harmonic mean of Precision & Recall\n",
    "    - Use Case: Unbalanced datasets\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- Confusion Matrix:\n",
    "    - Meaning: Table showing TP, TN, FP, FN counts\n",
    "    - Use Case: To see class-wise errors\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
