{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fe76cc",
   "metadata": {},
   "source": [
    "### Classification Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n",
      "precision: 0.6666666666666666\n",
      "recallScore: 0.4\n",
      "f1Score: 0.5\n",
      "confusionMetricsTable: [[2 1]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 0, 1, 0, 1, 1]\n",
    "y_pred = [1, 0, 0, 0, 0, 1, 0, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recallScore = recall_score(y_true, y_pred)\n",
    "f1Score = f1_score(y_true, y_pred)\n",
    "confusionMetricsTable = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"accuracy:\", accuracy)                                   \n",
    "print(\"precision:\", precision)\n",
    "print(\"recallScore:\", recallScore)\n",
    "print(\"f1Score:\", f1Score)\n",
    "print(\"confusionMetricsTable:\", confusionMetricsTable)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04840881",
   "metadata": {},
   "source": [
    "Confuction Metrics Table\n",
    "|               | **Predicted: 1**          | **Predicted: 0**             |\n",
    "| ------------- | ------------------------- | ---------------------------- |\n",
    "| **Actual: 1** | **TP = 2** (indices 0, 7) | **FN = 3** (indices 2, 4, 6) |\n",
    "| **Actual: 0** | **FP = 1** (index 5)      | **TN = 2** (indices 1, 3)    |\n",
    "\n",
    "Metrics Table: \n",
    "| Metric        | Formula                         | Calculation             | **Result** |\n",
    "| ------------- | ------------------------------- | ----------------------- | ---------- |\n",
    "| **Accuracy**  | (TP + TN) / (TP + TN + FP + FN) | (2 + 2) / 8             | **0.5**    |\n",
    "| **Precision** | TP / (TP + FP)                  | 2 / (2 + 1)             | **0.67**   |\n",
    "| **Recall**    | TP / (TP + FN)                  | 2 / (2 + 3)             | **0.4**    |\n",
    "| **F1-Score**  | 2 × (P × R) / (P + R)           | 2×(0.67×0.4)/(0.67+0.4) | **0.5**    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951c3ff",
   "metadata": {},
   "source": [
    "- Accuracy:\n",
    "    - Meaning: % of correct predictions\n",
    "    - Use Case: Balanced datasets\n",
    "    - Ideal Value: 1.0 (100%)\n",
    "\n",
    "- Precision:\n",
    "    - Meaning: Of all predicted positives, how many are actually positive\n",
    "    - Use Case: When False Positives matter (e.g. spam filter)\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- Recall (Sensitivity):\n",
    "    - Meaning: Of all actual positives, how many did we catch\n",
    "    - Use Case: When False Negatives matter (e.g. disease detection)\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- F1-Score:\n",
    "    - Meaning: Harmonic mean of Precision & Recall\n",
    "    - Use Case: Unbalanced datasets\n",
    "    - Ideal Value: 1.0\n",
    "\n",
    "- Confusion Matrix:\n",
    "    - Meaning: Table showing TP, TN, FP, FN counts\n",
    "    - Use Case: To see class-wise errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040402e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
